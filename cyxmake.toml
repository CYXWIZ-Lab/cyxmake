# CyxMake Configuration File
# Copy this file to cyxmake.toml and configure your settings

# =============================================================================
# AI Provider Configuration
# =============================================================================
# CyxMake supports multiple AI providers. Configure one or more providers
# and set the default one to use.

[ai]
# Default provider to use (must match a provider name below)
default_provider = "custom"

# Fallback provider if the default fails (optional)
fallback_provider = "openai"

# Request timeout in seconds (increase for slow local models)
timeout = 300

# Maximum tokens for responses
max_tokens = 1024

# Temperature for generation (0.0 = deterministic, 1.0 = creative)
temperature = 0.7

# =============================================================================
# Provider: Ollama (Local)
# =============================================================================
# Ollama runs models locally. Install from: https://ollama.ai
# No API key required, just run: ollama run llama2

[ai.providers.ollama]
enabled = true
type = "ollama"
base_url = "http://localhost:11434"
model = "llama2"
# Alternative models: "codellama", "mistral", "mixtral", "phi", "gemma"

# =============================================================================
# Provider: OpenAI
# =============================================================================
# Get your API key from: https://platform.openai.com/api-keys

[ai.providers.openai]
enabled = true
type = "openai"
api_key = "${OPENAI_API_KEY}"  # Use environment variable
# api_key = "sk-..."           # Or hardcode (not recommended)
base_url = "https://api.openai.com/v1"
model = "gpt-4o-mini"
# Alternative models: "gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"

# =============================================================================
# Provider: Google Gemini
# =============================================================================
# Get your API key from: https://makersuite.google.com/app/apikey

[ai.providers.gemini]
enabled = false
type = "gemini"
api_key = "${GEMINI_API_KEY}"
base_url = "https://generativelanguage.googleapis.com/v1beta"
model = "gemini-1.5-flash"
# Alternative models: "gemini-1.5-pro", "gemini-pro"

# =============================================================================
# Provider: Anthropic Claude
# =============================================================================
# Get your API key from: https://console.anthropic.com/

[ai.providers.anthropic]
enabled = false
type = "anthropic"
api_key = "${ANTHROPIC_API_KEY}"
base_url = "https://api.anthropic.com/v1"
model = "claude-3-haiku-20240307"
# Alternative models: "claude-3-5-sonnet-20241022", "claude-3-opus-20240229"

# =============================================================================
# Provider: xAI Grok
# =============================================================================
# Get your API key from: https://x.ai

[ai.providers.grok]
enabled = false
type = "openai"  # Grok uses OpenAI-compatible API
api_key = "${XAI_API_KEY}"
base_url = "https://api.x.ai/v1"
model = "grok-beta"

# =============================================================================
# Provider: OpenRouter (Access multiple models)
# =============================================================================
# Get your API key from: https://openrouter.ai/keys
# OpenRouter provides access to many models through one API

[ai.providers.openrouter]
enabled = true
type = "openai"  # OpenRouter uses OpenAI-compatible API
api_key = "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"
base_url = "https://openrouter.ai/api/v1"
model = "meta-llama/llama-3.1-8b-instruct:free"
# Popular models:
# - "anthropic/claude-3.5-sonnet"
# - "google/gemini-pro-1.5"
# - "meta-llama/llama-3.1-70b-instruct"
# - "mistralai/mixtral-8x7b-instruct"
# - "openai/gpt-4o-mini"

# Optional: Add site info for OpenRouter rankings
[ai.providers.openrouter.headers]
"HTTP-Referer" = "https://cyxmake.dev"
"X-Title" = "CyxMake"

# =============================================================================
# Provider: Groq (Fast inference)
# =============================================================================
# Get your API key from: https://console.groq.com/

[ai.providers.groq]
enabled = false
type = "openai"  # Groq uses OpenAI-compatible API
api_key = "${GROQ_API_KEY}"
base_url = "https://api.groq.com/openai/v1"
model = "llama-3.1-70b-versatile"
# Alternative models: "llama-3.1-8b-instant", "mixtral-8x7b-32768"

# =============================================================================
# Provider: Together AI
# =============================================================================
# Get your API key from: https://api.together.xyz/

[ai.providers.together]
enabled = false
type = "openai"  # Together uses OpenAI-compatible API
api_key = "${TOGETHER_API_KEY}"
base_url = "https://api.together.xyz/v1"
model = "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"

# =============================================================================
# Provider: Local llama.cpp
# =============================================================================
# Use a local GGUF model with llama.cpp

[ai.providers.local]
enabled = false
type = "llamacpp"
model_path = "/path/to/model.gguf"
# model_path = "C:\\models\\llama-2-7b-chat.Q4_K_M.gguf"  # Windows
context_size = 4096
gpu_layers = 0  # Number of layers to offload to GPU (0 = CPU only)
threads = 4     # Number of CPU threads

# =============================================================================
# Provider: Custom OpenAI-Compatible Server
# =============================================================================
# Use any server that implements the OpenAI API (LM Studio, vLLM, etc.)

[ai.providers.custom]
enabled = true
type = "openai"
api_key = "not-needed"  # Many local servers don't need a key
base_url = "http://localhost:1234/v1"  # LM Studio default
model = "openai/gpt-oss-20b"

# =============================================================================
# Project Settings
# =============================================================================

[project]
# Project name (auto-detected if not set)
# name = "my-project"

# Primary language (auto-detected if not set)
# language = "c++"

# Build system (auto-detected if not set)
# build_system = "cmake"

[build]
# Default build type
type = "Debug"

# Build directory
build_dir = "build"

# Number of parallel jobs (0 = auto)
parallel_jobs = 0

# Clean before building
clean_first = false

# =============================================================================
# Permission Settings
# =============================================================================

[permissions]
# Auto-approve safe operations without asking
auto_approve_read = true
auto_approve_build = true
auto_approve_list = true

# Always ask for dangerous operations
always_confirm_delete = true
always_confirm_install = true
always_confirm_command = true

# Remember user choices for this session
remember_choices = true

# =============================================================================
# Logging
# =============================================================================

[logging]
# Log level: debug, info, warning, error
level = "info"

# Enable colored output
colors = true

# Show timestamps
timestamps = false

# Log file (optional)
# file = ".cyxmake/cyxmake.log"
